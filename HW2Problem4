import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt

# (a) Load the dataset
data = pd.read_csv("winequality-red.csv", sep=';')

# (b) Split the dataset into train, validation, and test sets
train_data, temp_data = train_test_split(data, test_size=0.4, random_state=42)
val_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42)

X_train = train_data.iloc[:, :-1].values
y_train = train_data.iloc[:, -1].values
X_val = val_data.iloc[:, :-1].values
y_val = val_data.iloc[:, -1].values
X_test = test_data.iloc[:, :-1].values
y_test = test_data.iloc[:, -1].values

# Add a bias term (intercept) to the features
X_train = np.c_[np.ones(X_train.shape[0]), X_train]
X_val = np.c_[np.ones(X_val.shape[0]), X_val]
X_test = np.c_[np.ones(X_test.shape[0]), X_test]

# (c) Closed-form solution for linear regression
def closed_form_solution(X, y):
    y = np.array(y, dtype=float)  # Ensure y is a numpy array of floats
    return np.linalg.inv(X.T @ X) @ X.T @ y

# Sum-of-squares error function
def sum_of_squares_error(X, y, w):
    predictions = X @ w
    errors = y - predictions
    return 0.5 * np.sum(errors ** 2)

w_closed_form = closed_form_solution(X_train, y_train)

# Calculate the sum-of-squares error for the closed-form solution
sse_train = sum_of_squares_error(X_train, y_train, w_closed_form)
sse_test = sum_of_squares_error(X_test, y_test, w_closed_form)

print(f"Sum-of-Squares Error (Train, Closed-form): {sse_train:.4f}")
print(f"Sum-of-Squares Error (Test, Closed-form): {sse_test:.4f}")

# Predictions for train data
y_train_pred = X_train @ w_closed_form

# (d) Plot actual vs predicted target values
plt.figure(figsize=(8, 6))
plt.scatter(y_train, y_train_pred, alpha=0.7, edgecolors='k')
plt.plot([min(y_train), max(y_train)], [min(y_train), max(y_train)], color='red', linestyle='--')
plt.xlabel("Actual Values")
plt.ylabel("Predicted Values")
plt.title("Actual vs Predicted Values (Train Data)")
plt.show()

# (e) Root Mean Square Error (RMSE)
def rmse(y_true, y_pred):
    return np.sqrt(np.mean((y_true - y_pred) ** 2))

train_rmse = rmse(y_train, y_train_pred)
y_test_pred = X_test @ w_closed_form
test_rmse = rmse(y_test, y_test_pred)

print(f"Train RMSE (Closed-form): {train_rmse:.4f}")
print(f"Test RMSE (Closed-form): {test_rmse:.4f}")

# (f) Least-Mean-Squares (LMS) algorithm
def lms(X, y, stepsize=0.01, epochs=1000):
    np.random.seed(42)
    w = np.random.randn(X.shape[1])  # Random initialization
    n = len(y)
    for epoch in range(epochs):
        for i in range(n):
            gradient = (X[i] @ w - y[i]) * X[i]
            w -= stepsize * gradient
    return w

w_lms = lms(X_train, y_train, stepsize=0.0001, epochs=2000)

# Predictions for train and test data (LMS)
y_train_pred_lms = X_train @ w_lms
y_test_pred_lms = X_test @ w_lms

# (g) Report RMSE for LMS
train_rmse_lms = rmse(y_train, y_train_pred_lms)
test_rmse_lms = rmse(y_test, y_test_pred_lms)

print(f"Train RMSE (LMS): {train_rmse_lms:.4f}")
print(f"Test RMSE (LMS): {test_rmse_lms:.4f}")
