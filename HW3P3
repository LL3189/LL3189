import numpy as np

def sigmoid(z):
    return 1 / (1 + np.exp(-z))

def compute_loss(y_true, y_pred):
    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))

def logistic_regression_sgd(X, y, batch_size=32, learning_rate=0.01, max_iters=1000):
    n_samples, n_features = X.shape
    weights = np.random.randn(n_features)
    bias = np.random.randn()
    
    for _ in range(max_iters):
        indices = np.random.permutation(n_samples)
        X_shuffled, y_shuffled = X[indices], y[indices]
        
        for start in range(0, n_samples, batch_size):
            end = min(start + batch_size, n_samples)
            X_batch, y_batch = X_shuffled[start:end], y_shuffled[start:end]
            
            linear_model = np.dot(X_batch, weights) + bias
            y_pred = sigmoid(linear_model)
            
            grad_w = np.dot(X_batch.T, (y_pred - y_batch)) / batch_size
            grad_b = np.mean(y_pred - y_batch)
            
            weights -= learning_rate * grad_w
            bias -= learning_rate * grad_b
            
    return weights, bias
